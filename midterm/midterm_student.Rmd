# JOUR405: Statistics for Journalists
## Midterm Exam - Spring 2025

Name: Tess Phillips

For this exam, you'll analyze several datasets using R and the statistical concepts we've covered in class. Load the tidyverse before beginning, then complete each task. Write your code in the provided blocks and answer the questions in complete sentences. Start by loading the tidyverse and any other libraries you think you might need.

```{r}
library(tidyverse)
```

## Part 1: Restaurant Health Inspections (15 points)

You want to understand how restaurants in Montgomery County are performing on health inspections. The first dataset contains restaurant health inspection scores for restaurants in Montgomery County. The dataset includes the name of the establishment, the number of points for critical and non-critical areas, the total points, maximum points possible and the compliance score and grade. Load the data from: `https://raw.githubusercontent.com/dwillis/jour405/refs/heads/main/data/montco_inspections.csv` and complete these tasks:

### Tasks:
1. Calculate the mean and standard deviation of compliance scores (5 points) 
2. Create a histogram of the compliance scores with a vertical line showing the mean (5 points)
3. Write 2-3 sentences interpreting what the standard deviation and histogram tell us about the distribution of compliance scores. What would be newsworthy about this distribution? What's the story here? (5 points).

With this data set, the range of compliance scores can go from 0-100. Because we have such a high mean of 96, we can see that a really large portion of the restaurants passed inspection with high compliance scores. The standard deviation being so low illustrates to use that the data is not very spread out, which is supported by the histogram. I think the story here could be a further investigation into the restaurants that did not pass inspection, and finding out why they have such low compliance scores. How does one obtain such a low compliance score and was is the measure for that?

```{r}
inspection_data <- read_csv("https://raw.githubusercontent.com/dwillis/jour405/refs/heads/main/data/montco_inspections.csv")
inspection_data |> summarize(mean = mean(compliance_score), sd = sd(compliance_score))

```
```{r}
inspection_data |> 
  ggplot() +
  geom_histogram(aes(x = compliance_score), binwidth = 12) +
  geom_vline(aes(xintercept = mean(compliance_score)), color = "red", linetype = "dashed", size = 1) 
```


## Part 2: High School Athletics (25 points)

You are reporting a story about high school sports participation in Maryland and want to see if there are differences between boys and girls. The second dataset shows participation numbers in high school sports across Maryland counties in 2024, broken down by sport and sex. Load the data from: `https://raw.githubusercontent.com/example/md_hs_sports_2024.csv` and complete these tasks:

### Tasks:
1. Calculate the correlation between boys' and girls' participation (5 points)
2. Add two columns called total and girls_pct using mutate(), with the total adding together boys and girls and girls_pct being the percentage of the total represented by girls participants. (5 points)
3. Create a scatterplot showing this relationship, adding a line of best fit (5 points)
4. In 2-3 sentences, explain what the correlation coefficient and scatterplot reveal about equity in Maryland high school sports participation. How do you interpret the school districts that are below the line vs those that are above? Which school districts are most worth examining further, and why? (10 points)

The scatterplot shows us that the less students there are, the higher percentage of girls there are participating in high school athletics. Furthermore based on the graph and the correlation coefficient, we can assume the equity in MD schools is really not that good. The schools that fall above the line of best fit must be more equitable, given their higher percentages of inclusion. I think that the school that falls exactly on the line of best fit with the highest total needs to be examined. It is an outlier because it has such a high population but also a decent amount of participating girls. We would need to chat with them and learn more data about their athletics. 

```{r}
hs_athletics <- read_csv("https://raw.githubusercontent.com/dwillis/jour405/refs/heads/main/data/md_hs_participation.csv")
hs_athletics |> summarize(correlation = cor(boys, girls, method="pearson"))
hs_athletics_data <- hs_athletics |> mutate(total = boys + girls, girls_pct = girls/total*100)
```
```{r}
hs_athletics_data |> 
  ggplot() +
  geom_point(aes(x=total, y=girls_pct)) +
  geom_smooth(aes(x=total, y=girls_pct), method = "lm") +
  ggtitle("total vs. girls_pct")
```


## Part 3: Public Transit Ridership (20 points)

You are investigating public transit ridership in the Washington, D.C. area and want to understand the patterns of daily bus and rail ridership. The third dataset contains daily bus and rail ridership totals from WMATA for the past year. Load the data from https://raw.githubusercontent.com/dwillis/jour405/refs/heads/main/data/wmata_daily.csv and do the following:

### Tasks:
1. Calculate the average bus and rail ridership and standard deviation using summarize() (5 points)
2. Using the process you used in class, take a random sample daily ridership numbers and calculate the sample means and deviations for bus and rail. The number in the sample is up to you, but explain why you chose what you did. Compare this to the stats you generated in step 1. (5 points)


3. Using group_by() and summarize(), calculate the means for bus and rail ridership for each weekday. Describe the overall pattern of ridership for bus and rail - which days stand out and why? Are there differences between bus and rail in the standard deviation values? (10 points)


```{r}
transit <- read_csv("https://raw.githubusercontent.com/dwillis/jour405/refs/heads/main/data/wmata_daily.csv")
transit |> summarize(mean = mean(bus), sd = sd(bus))
transit |> summarize(mean = mean(rail), sd = sd(rail))

```
```{r}
transit <-  total |> 
  sample_n(100)
sample_100 |> summarize(mean = mean(bus), sd = sd(bus))
sample_100 |> summarize(mean = mean(rail), sd = sd(rail))
```



## Part 4: Maryland Car Theft Rates (20 points)

Your editor has assigned you a story about car thefts in Maryland and wants you to analyze the data to find out which counties have the highest rates. The fourth dataset contains car theft statistics for Maryland counties in 2023 and population. Load the data from: `https://raw.githubusercontent.com/dwillis/jour405/refs/heads/main/data/md_car_thefts.csv` and complete the following tasks:

```{r}
car_thefts <- read_csv("https://raw.githubusercontent.com/dwillis/jour405/refs/heads/main/data/md_car_thefts.csv")
```


### Tasks:
1. Using mutate, add a column that calculates the rate of car thefts for each county - you need to choose the per capita rate (5 points)
2. Calculate the median car theft rate and the total number of car thefts statewide. Which counties have rates above the median, and what percentage of all car thefts occur in those counties? (5 points)
3. Write 2-3 sentences describing what these calculations reveal about the distribution of car thefts in Maryland. What's the lede of a story about your findings? (10 points)

With very few exceptions, car thefts in each county saw some pretty big increases this year. Only a few saw decreases, and even then the decrease was not significant. I would probably make my lede "Twenty counties in Maryland saw significant increases in car thefts between 2022 and 2023."


```{r}
car_theft_rates <- car_thefts |> mutate(thefts_per_county = 2023/population)
median <- median(car_thefts)
```

## Part 5: Data Analysis Scenario (20 points)

You receive a tip that local emergency response times have gotten significantly worse over the past year. You obtain monthly data on response times for police, fire and ambulance calls.

Write 3-4 sentences (no code!) explaining:
1. What statistical measures would you calculate to verify this claim? (10 points)

To verify this claim, I would need to see all response times for the past year, as well as probably two years (or more) before that. Verifying this claim could be made easy by finding out mean repsonse times for each year and comparing them to the current ones. If the mean of this past year is significantly lower than others, the claim is verified. You could also utilize standard deviation to see how spread out the data is. In this case, response times should not be spread out, so standard deviation could be useful. 

2. What visualizations would help readers understand the trends? (5 points)

I think that two histograms side by side comparing this year and last years' response times could be visually helpful to readers. I also think that a scatterlot could be of use. Especially if the line of best fit represents what the response times should be and the data points indicate what they actually are. 

3. What additional context or data would you need to make this a complete story? (5 points)

To make this a true and complete trends story, I would want to see the data going pretty far back, maybe even for the past 5 years or so. Then we would be able to examine if it has been a steady decrease in response time or is this just a new trend. I would also like to see a list of when everyone who works at the station has been hired. Is this decrease due to laziness from long standing employees? Or is it carelessness of the new hires? I would also need to interview people working and the ones who are responsible for slow response times. Were they dealing with other serious matters, or again, is it just laziness?

### Submission Instructions
- Save your work frequently
- Make sure all code blocks run without errors
- Provide clear explanations for your analytical choices
- Before submitting, clear your environment and run the entire notebook

Good luck!
